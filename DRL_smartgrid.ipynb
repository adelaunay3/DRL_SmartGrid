{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kjMxnidAXK3j"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vtVtZ4H1pPq8"
   },
   "outputs": [],
   "source": [
    "ACTIONS = [\"charge\", \"discharge\", \"generator\", \"discharge + generator\", \"nothing\"]\n",
    "NB_ACTION = len(ACTIONS)\n",
    "EPS = 5e-2\n",
    "GAMMA = 1.0\n",
    "\n",
    "class State:\n",
    "    def __init__(self):\n",
    "        self.battery = 0.0\n",
    "        self.panelProd = 0.0\n",
    "        self.consumption = 0.0\n",
    "        self.price = 0.0\n",
    "        self.daytime = 0.0\n",
    "        self.row = 0\n",
    "  \n",
    "    def toArray(self):\n",
    "        return np.array([self.battery, self.panelProd, self.consumption, self.price,0]) #self.daytime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1DUpDwwJj0LZ"
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        # load data (csv)\n",
    "        #importation données\n",
    "        df = pandas.read_csv(\"select_data.csv\", sep = ';', header = 0)\n",
    "        # TODO: transformer daytime en float\n",
    "\n",
    "        self.data=df.values\n",
    "        # for i in range(len(self.data)):\n",
    "        #     self.data[i,3]=self.data[i,3]*8000\n",
    "        # print(self.data[500])\n",
    "        self.initState()\n",
    "        self.batteryCapacity = 0.0\n",
    "        self.generatorCapacity = 0.0\n",
    "\n",
    "        self.co2Price = 0.0      #price per ton of CO2 (mean price from the european market)\n",
    "        self.co2Generator = 0.0   #ton of CO2 generated per Wh from the diesel generator\n",
    "        self.co2Market = 0.0        #ton of CO2 generated per Wh from the national power market (danish)\n",
    "\n",
    "        self.chargingCost = 0.0\n",
    "        self.dischargingCost = 0.0\n",
    "        #self.solarCost = 0.0\n",
    "        self.generatorCost = 0.0\n",
    "\n",
    "        self.chargingYield = 1.0\n",
    "        self.dischargingYield = 1.0\n",
    "\n",
    "\n",
    "    def initState(self):\n",
    "        self.currentState = State()\n",
    "        self.currentState.row = np.random.randint(0, len(self.data)) # Deuxième valeur à modifier en fonction du nombre de steps réalisés par épisode\n",
    "        row = self.currentState.row\n",
    "        self.currentState.daytime = self.data[row,1]\n",
    "        self.currentState.panelProd = self.data[row,3]\n",
    "        self.currentState.price = self.data[row,4]\n",
    "        self.currentState.consumption = self.data[row,5]\n",
    "\n",
    "    def act(self, action):\n",
    "        totalProd = self.currentState.panelProd\n",
    "        diffProd = totalProd - self.currentState.consumption\n",
    "\n",
    "        cost=0.0\n",
    "\n",
    "        if action == \"charge\":\n",
    "            if (diffProd>0):\n",
    "                charge = min(diffProd,(self.batteryCapacity-self.currentState.battery)/self.chargingYield)\n",
    "                self.currentState.battery+=charge*self.chargingYield\n",
    "                diffProd-=charge\n",
    "                cost+=charge*self.chargingCost\n",
    "\n",
    "\n",
    "        elif action == \"discharge\":\n",
    "            if (diffProd<0):\n",
    "                discharge = max(diffProd/self.dischargingYield,-self.currentState.battery)\n",
    "                self.currentState.battery+=discharge\n",
    "                diffProd-=discharge*self.dischargingYield\n",
    "                cost+=abs(discharge*self.dischargingCost)\n",
    "\n",
    "\n",
    "        elif action == \"generator\":\n",
    "              if (diffProd<0):\n",
    "                generate=min(-diffProd, self.generatorCapacity)\n",
    "                diffProd+=generate\n",
    "                cost+=generate*self.generatorCost\n",
    "\n",
    "\n",
    "        elif action == \"discharge + generator\":\n",
    "            if (diffProd<0):\n",
    "                discharge = max(diffProd/self.dischargingYield,-self.currentState.battery)\n",
    "                self.currentState.battery+=discharge\n",
    "                diffProd-=discharge*self.dischargingYield\n",
    "                cost+=abs(discharge*self.dischargingCost)\n",
    "\n",
    "\n",
    "            if (diffProd<0):\n",
    "                generate=min(-diffProd, self.generatorCapacity)\n",
    "                diffProd+=generate\n",
    "                cost+=generate*self.generatorCost\n",
    "\n",
    "\n",
    "        cost+=diffProd*self.currentState.price\n",
    "\n",
    "        #UPDATE SELF.PANELPROD, PRICE, CONSUMPTION, DAYTIME according to the dataset\n",
    "        row = self.currentState.row + 1\n",
    "        self.currentState.daytime = self.data[row,1]\n",
    "        self.currentState.panelProd = self.data[row,3]\n",
    "        self.currentState.price = self.data[row,4]\n",
    "        self.currentState.consumption = self.data[row,5]\n",
    "        self.currentState.row = row\n",
    "\n",
    "        return -cost, self.currentState\n",
    "\n",
    "\n",
    "    def getState(self):\n",
    "        return self.currentState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kYzMKL07p_3d"
   },
   "outputs": [],
   "source": [
    "def DQN(n_neurons, input_size):\n",
    "    model = tf.keras.Sequential(name=\"DQN\")\n",
    "    model.add(layers.Dense(n_neurons, input_shape=(input_size,), bias_initializer=\"glorot_normal\", \n",
    "                           kernel_initializer=\"glorot_normal\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.Dense(n_neurons, bias_initializer=\"glorot_normal\", kernel_initializer=\"glorot_normal\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    \n",
    "    model.add(layers.Dense(units = 1, bias_initializer=\"glorot_normal\", kernel_initializer=\"glorot_normal\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ktQ41-LDp_3i"
   },
   "outputs": [],
   "source": [
    "def predict(model, state, action):\n",
    "    input_model = np.array([0.0]*NB_ACTION + list(state.toArray()))\n",
    "\n",
    "    if action == \"charge\":\n",
    "        input_model[0] = 1.0\n",
    "    elif action == \"discharge\":\n",
    "        input_model[1] = 1.0\n",
    "    elif action == \"generator\":\n",
    "        input_model[2] = 1.0\n",
    "    elif action == \"discharge + generator\":\n",
    "        input_model[3] = 1.0\n",
    "    else:\n",
    "        input_model[4] = 1.0\n",
    "\n",
    "    #print(action)\n",
    "    #print(input_model)\n",
    "    res= model(np.array([input_model]))\n",
    "\n",
    "    #print(input_model, res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zc31kqaJp_3o"
   },
   "outputs": [],
   "source": [
    "def policy(model, state):\n",
    "    q_value = [predict(model, state, action) for action in ACTIONS]\n",
    "    prob = np.ones(NB_ACTION) * EPS / NB_ACTION\n",
    "    prob[np.argmax(q_value)] += 1.0 - EPS\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0QTzmmhHp_3s"
   },
   "outputs": [],
   "source": [
    "def loss(model, transitions_batch):\n",
    "    y = []\n",
    "    q = []\n",
    "    for state, action, reward, next_state in transitions_batch:\n",
    "        q_value = [predict(model, next_state, a) for a in ACTIONS]\n",
    "        best_next_action = np.argmax(q_value)\n",
    "        y.append(reward + GAMMA * q_value[best_next_action])\n",
    "        q.append(predict(model, state, action))\n",
    "    \n",
    "    return tf.reduce_mean(tf.square(q - tf.stop_gradient(y)), name=\"loss_mse_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bDuz3yxgp_3x"
   },
   "outputs": [],
   "source": [
    "def train_step(model, transitions_batch):\n",
    "    #batch_size = transitions_batch.shape[0]\n",
    "    \n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        disc_loss = loss(model, transitions_batch)\n",
    "\n",
    "    gradients = disc_tape.gradient(disc_loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "s5estrpZp_30",
    "outputId": "0a852cd0-f9be-48af-9f10-d53b3f89080d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"DQN\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                110       \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 10)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                110       \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 10)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 231\nTrainable params: 231\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = Env()\n",
    "env.initState()\n",
    "\n",
    "nb_episodes = 100\n",
    "nb_steps = 10\n",
    "batch_size = 10\n",
    "\n",
    "DQN_model = DQN(n_neurons=10, input_size=10)\n",
    "DQN_model.summary()\n",
    "#print(DQN_model(np.array([[0.0]*10])))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "state = env.currentState\n",
    "\n",
    "replay_memory = []\n",
    "replay_memory_init_size = 100\n",
    "\n",
    "for i in range(replay_memory_init_size):\n",
    "    action_probs = policy(DQN_model, state)\n",
    "    action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "    reward, next_state = env.act(action)\n",
    "    replay_memory.append((state, action, reward, next_state))\n",
    "    state=next_state\n",
    "\n",
    "for i_episode in range(nb_episodes):\n",
    "    env.initState()\n",
    "    state = env.currentState\n",
    "    loss_episode = None\n",
    "    #print(i_episode)\n",
    "\n",
    "    for step in range(nb_steps):\n",
    "        action_probs = policy(DQN_model, state)\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        reward, next_state = env.act(action)\n",
    "        \n",
    "        replay_memory.pop(0)\n",
    "        replay_memory.append((state, action, reward, next_state))\n",
    "\n",
    "        samples = random.sample(replay_memory, batch_size)\n",
    "\n",
    "        train_step(DQN_model, samples)\n",
    "        state=next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2017-08-15 20:40:00+00:00\n"
     ]
    }
   ],
   "source": [
    "conso=[]\n",
    "prod=[]\n",
    "battery=[]\n",
    "price=[]\n",
    "actions=[]\n",
    "cost=[]\n",
    "env.initState()\n",
    "print(env.currentState.daytime)\n",
    "for i in range(10000):\n",
    "    action_probs = policy(DQN_model, state)\n",
    "    action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "    reward, next_state = env.act(action)\n",
    "    replay_memory.append((state, action, reward, next_state))\n",
    "    conso.append(state.consumption)\n",
    "    prod.append(state.panelProd)\n",
    "    battery.append(state.battery)\n",
    "    price.append(state.price)\n",
    "    cost.append(-reward)\n",
    "    actions.append(action)\n",
    "    state=next_state\n",
    "    \n",
    "    #if (i%100==0):\n",
    "    #    print(i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_smartgrid(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}